# Story 4.1: Relevance Scoring Engine

## Status
Approved

## Story
**As the** discovery agent scoring service,
**I want** to assign a relevance score to each normalized item,
**so that** only high-value nuggets reach reviewers.

## Acceptance Criteria
1. Relevance score calculated as weighted sum of keyword match, recency decay, and source weighting (RSS feeds, YouTube channels) with output normalized between 0 and 1.
2. Default weights and thresholds configurable per client via the existing configuration service without redeploy.
3. Scoring output persisted using the `discovery_scores` table (score + component breakdown + applied threshold) and linked `discovery_items.status` transitions to `scored`.
4. Items below the configured threshold transition `discovery_items.status` to `suppressed`, remain queryable for auditing, and surface component metadata for review.

## Tasks / Subtasks
- [ ] Implement `DiscoveryScoringAgent` logic in `packages/agents-server/src/agents/discovery-scoring.ts`, loading weight providers via the configuration service helpers (AC 1, AC 2).
- [ ] Extend `packages/agents-server/src/services/discovery-repository.ts` and related Drizzle adapters to upsert `discovery_scores` entries and update `discovery_items.status` between `scored`/`suppressed` per threshold (AC 3, AC 4).
- [ ] Ship configuration helper updates (`server/utils/client-config/feature-flags.ts`, `packages/agents-server/src/utils/feature-flags.ts`) and regression tests before wiring `DiscoveryScoringAgent` so new weights/thresholds resolve correctly (AC 2).
- [ ] Ensure suppression flow removes affected items from reviewer queues and emits queue update events so operators are never left with orphaned work (AC 4).
- [ ] Ship sequential Drizzle migrations in `packages/db/migrations` that introduce `discovery_scores` (structure first, then constraints/backfill hooks) while preserving existing discovery tables (AC 3).
- [ ] Implement an idempotent backfill job or script that initializes `discovery_scores` for pre-existing `discovery_items` before flipping feature flags (AC 3, AC 4).
- [ ] Wire configurability for thresholds/weights through existing client configuration endpoints (`server/utils/client-config/feature-flags.ts` + related API) and ensure updates propagate without redeploy (AC 2).
- [ ] Emit `discovery.score.complete` AgentEvent payloads via `signalAgentEvent`, including score components and status outcome (AC 3).
- [ ] Implement resilient configuration fallback (cached defaults, sensible retry/backoff) when the configuration service is unavailable so scoring can continue safely (AC 2).
- [ ] Write unit and integration tests covering scoring math, repository persistence, SSE emission, and configuration update paths (AC 1, AC 2, AC 3, AC 4).
- [ ] Update `README.md` with scoring environment prerequisites, Redis/pub-sub bootstrap notes, and local quickstart commands as part of Definition of Done (supports AC 2).
- [ ] Document rollback triggers (telemetry thresholds, queue anomalies), procedures for disabling scoring via feature flags/config service, and schema rollback/backfill recovery steps (AC 3, AC 4).

## Dev Notes
- Reference `docs/prd/epic-discovery-scoring-dedup.md` for required scoring inputs (keyword match, recency decay, source weighting, traction signals) and objective thresholds.
- Leverage the configuration patterns in `docs/architecture/config-management.md` to fetch and update per-client weights/thresholds (cache invalidation, pub/sub events).
- Use the persistence/data model guidance in `docs/architecture/discovery-agent-backend.md#data-model-additions` to keep `discovery_items` and `discovery_scores` aligned.
- Follow Drizzle sequencing best practices (`docs/architecture/source-tree.md#5`, `docs/architecture/discovery_agent_backend/data-model-additions.md`) by creating a schema migration for `discovery_scores` and a subsequent data migration that backfills existing `discovery_items` before enabling the feature flag.
- The backfill path should reuse the discovery scoring queue: select items lacking a `discovery_scores` row, reset them to `pending_scoring`, and enqueue `DiscoveryScoringAgent` runs (or insert default rows) so deployment does not strand legacy data; make the job repeatable and safe to re-run.
- Sequence configuration helper work ahead of agent wiring: land weighting/threshold support and tests in `server/utils/client-config/feature-flags.ts` and `packages/agents-server/src/utils/feature-flags.ts`, confirm cache invalidation behaviour, then update `DiscoveryScoringAgent` to consume the new interfaces. This prevents runtime mismatches noted in the PO checklist.
- Define resilience for configuration outages: latch the last-known weights in Redis/in-memory, fall back to baseline weights from `docs/prd/epic-discovery-scoring-dedup.md`, and retry helper lookups with exponential backoff before failing a scoring run. Document the behaviour in code comments and README troubleshooting.
- Deployment pipeline: migrations/backfill → deploy configuration helper updates with `DISCOVERY_ENABLE=false` → deploy scoring agent changes → run smoke tests → enable the feature flag for a single pilot client (canary) → expand once telemetry (SSE, queue health) stays green. Mirror the sequence in release notes per `docs/discovery-agent-dev-setup.md#6-release--rollback-checklist`.
- Define rollback triggers: if scoring precision drops below PRD targets (e.g., reviewer rejection rate >5% in 1h) or queue anomalies occur (suppressed items resurfacing), immediately disable scoring via configuration helpers (`DiscoveryScoringAgent` should respect `DISCOVERY_SCORING_ENABLED` flag). Document how to revert schema/data (`pnpm run db:rollback`, backfill replays) and restore pre-scoring statuses using the earlier SQL snippet.
- Before transitioning an item to `suppressed`, reconfirm it is not visible in reviewer backlogs per `docs/prd/epic-discovery-brief-dashboard.md`—remove or update any `review_queue` references and emit a `discovery.queue.updated`/`discovery.score.complete` event so operators see the change in real time.
- Observability must follow `docs/architecture/discovery_agent_backend/observability-logging.md` (structured logging, pending queue alerts) when emitting scoring results.
- If a local or integration run corrupts `discovery_items.status` transitions, reset the affected records to their pre-run state before rerunning scoring to preserve legacy data expectations.
- Provide a simple rollback SQL snippet (e.g., `UPDATE discovery_items SET status = 'pending_scoring' WHERE status IN ('scored','suppressed') AND updated_at >= :testWindow;`) in the deployment checklist so engineers can revert test-applied status changes quickly after a failed scoring deployment.
- Add an environment setup addendum aligned with `docs/architecture/tech-stack.md#4` and `docs/architecture/discovery-agent-backend.md#context-&-reuse` that lists required services (Neon Postgres, Redis cache/pub-sub, scoring agents runtime) and env vars consumed by the scoring path. Ensure these remote-environment expectations land in `README.md` as part of the Definition of Done.
- Capture implementation notes (schema sequence, config helpers, SSE contracts) in `docs/discovery-scoring-implementation-notes.md` and link reviewer guidance via `docs/discovery-scoring-reviewer-guide.md`.

### Testing
- Unit: cover scoring component calculations and normalization edge cases in `packages/agents-server/__tests__/discovery`.
- Unit: verify repository persistence updates both `discovery_scores` and `discovery_items.status` transitions.
- Integration: run pipeline test from normalized item ingestion through scoring agent, including emitted `discovery.score.complete` SSE payload.
- Integration: exercise configuration update endpoint to ensure new weights/thresholds apply without redeploy and cache invalidation propagates.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-03-29 | 0.1 | Draft for Epic: Scoring & Deduplication. | PM |

## Dev Agent Record
_Not yet worked._

## QA Results
_Not yet reviewed._
