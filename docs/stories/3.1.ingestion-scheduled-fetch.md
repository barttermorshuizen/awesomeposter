# Story 3.1: Scheduled Source Fetching

## Status
Approved

## Story
**As the** discovery ingestion service,
**I want** to fetch client sources on their configured cadence without overlapping runs,
**so that** normalized items stay fresh and reliable.

Refer to `docs/architecture/discovery_agent_backend/jobs-scheduling-and-throughput.md` for ingestion scheduler context and throughput expectations.
Story 3.3 finalizes the retry/backoff policy and is expected to land first; this story assumes its taxonomy and APIs are available.

## Acceptance Criteria
1. Each source stores a fetch interval (default 60 minutes) and scheduler respects it with no overlapping jobs per source.
2. Scheduler selects the appropriate fetch adapter for HTTP pages, RSS feeds, and YouTube channels/playlists based on the source configuration.
3. Concurrent fetches for different sources are allowed but capped to configurable worker pool size.
4. Scheduler records job start/end timestamps and success/failure to telemetry.
5. Failures (network, HTTP status ≥400, YouTube API errors) are logged with reason codes and retried according to backoff policy defined in Story 3.3.

## Tasks / Subtasks
- [x] Implement scheduler using existing job framework (`packages/agents-server` or `server/jobs`) with pluggable adapters per source type (AC 1, AC 2). extend packages/shared/src/discovery/ingestion.ts with new adapter modules under packages/shared/src/discovery/adapters/ and
  register them in the adapter map; no scheduler changes needed beyond wiring new source types into ingest-sources.ts.
- [x] Persist per-source next-run timestamp in `discovery_sources.next_fetch_at` and update atomically after each job using a single transaction that also records telemetry status (AC 1, AC 4).
- [x] Add worker pool configuration (env) and guard to prevent overrun (AC 3).
- [x] Emit telemetry events `ingestion.started` / `ingestion.completed` including source type, source id, duration, run id, and success flag; when failures occur, include `failure_reason` codes aligned with Story 3.3 backoff taxonomy and log structured error records (AC 4, AC 5).
- [x] Document scheduling behavior in runbook, highlighting RSS and YouTube cadence considerations (AC 1-5).

## Dev Notes
- Follow existing scheduling patterns documented in `docs/architecture/discovery_agent_backend/jobs-scheduling-and-throughput.md` (no new framework required).
- Ensure scheduler is resilient across restarts (persist state in DB, not memory).
- Use the existing `discovery_sources` table for both cadence (`fetch_interval_minutes`) and next run tracking (`next_fetch_at`); initialize missing values when new sources are created.
- On restart, seed the scheduler queue from `next_fetch_at` values to avoid duplicate executions; ensure transactional updates so telemetry + next-run commitment happen together.
- Telemetry contract: emit `ingestion.started` when a job dequeues (`source_id`, `source_type`, `run_id`, `scheduled_at`), and `ingestion.completed` on finish (`duration_ms`, `success`, `failure_reason?`, `retry_in_minutes?`). Log failures with structured context (`error_code`, HTTP status or API error). Align `failure_reason` codes with Story 3.3 backoff taxonomy (`network_error`, `http_4xx`, `http_5xx`, `youtube_quota`, etc.).
- Backoff taxonomy and retry escalation definitions live in `docs/stories/3.3.ingestion-retry-health.md`; reuse its `failure_reason` codes and retry API contracts without deviation.

### Testing
- Integration coverage should include:
  1. Happy-path cadence across multiple sources with staggered `next_fetch_at`, verifying single execution per interval and correct telemetry payloads.
  2. No-overlap guard where a long-running job blocks duplicate scheduling and logs a structured skip message.
  3. Worker-pool limit enforcement (e.g., pool size 2 with three simultaneous sources) showing queued execution order.
  4. Adapter selection validation for HTTP, RSS, and YouTube sources with stubs ensuring correct adapter dispatch.
  5. Failure path logging and telemetry when adapters return network/API errors, including `failure_reason` and `retry_in_minutes` aligned with Story 3.3.
  6. Restart recovery seeding scheduler state from `discovery_sources.next_fetch_at` without duplicates after process restart.
  7. Telemetry contract assertions confirming required fields (`source_id`, `source_type`, `run_id`, `scheduled_at`, `duration_ms`, `success`, optional `failure_reason`/`retry_in_minutes`).
  8. Edge cadence values (minimum/maximum and default 60 min) ensuring intervals persist correctly.
  9. Back-to-back success runs confirming `next_fetch_at` updates and telemetry continuity.
  10. Observability regression ensuring structured logs/metrics still emit as expected.
- Use fake timers and mocked adapters/HTTP responses to simulate long-running schedules and failure conditions.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-03-29 | 0.1 | Draft for Epic: Ingestion & Normalization. | PM |
| 2025-10-03 | 1.0 | Implemented scheduler, adapters, telemetry, and ops documentation. | Dev |

## Dev Agent Record

### Agent Model Used
- Codex (GPT-5) via CLI session

### Debug Log References
- npm run test:unit
- npm run test:unit -- --run --reporter=verbose server/jobs/__tests__/discovery-ingest-sources.spec.ts

### Completion Notes List
- Implemented concurrency-safe scheduler (`server/jobs/discovery/ingest-sources.ts`) with per-source claiming, telemetry emission, and transactional updates for cadence fields and run logs.
- Added shared ingestion adapter registry plus HTTP/RSS/YouTube adapters returning standardized failure reasons and retry hints for Story 3.3 backoff alignment.
- Extended discovery repository/data model with cadence columns, `discovery_ingest_runs` logging, and helper APIs to compute `next_fetch_at` atomically.
- Introduced Vitest coverage for scheduler success/failure paths and refreshed repository specs to cover cadence defaults and duplicate detection.
- Updated discovery runbook to document worker pool tuning, RSS defaults, and YouTube quota-friendly scheduling guidance.
- Coerced Drizzle string timestamps before telemetry emission, guaranteed completion finalization, and expanded ingestion job Vitest coverage for crash handling, worker queue limits, and retry telemetry propagation.
- Added fallback release path when completion persistence fails and backed it with a dedicated regression test to prevent sources from getting stuck in `running` status.

### File List
- docs/discovery-agent-dev-setup.md
- packages/db/drizzle/0012_discovery_ingestion_schedule.sql
- packages/db/drizzle/meta/_journal.json
- packages/db/drizzle/relations.ts
- packages/db/drizzle/schema.ts
- packages/db/src/schema.ts
- packages/shared/src/discovery.ts
- packages/shared/src/discovery/ingestion.ts
- packages/shared/src/discovery/adapters/http.ts
- packages/shared/src/discovery/adapters/rss.ts
- packages/shared/src/discovery/adapters/youtube.ts
- packages/shared/src/index.ts
- packages/shared/src/discovery-events.ts
- server/jobs/discovery/ingest-sources.ts
- server/jobs/__tests__/discovery-ingest-sources.spec.ts
- server/utils/discovery-repository.ts
- server/utils/discovery-telemetry.ts
- server/utils/__tests__/discovery-repository.spec.ts
- docs/stories/3.1.ingestion-scheduled-fetch.md

## QA Results

### Review Date: 2025-10-04

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
- ✅ Crash path addressed: `coerceTimestamp` converts Drizzle string timestamps before `toISOString()`, eliminating the runtime failure (`server/jobs/discovery/ingest-sources.ts:57-121`, `packages/db/drizzle/schema.ts:87-106`).
- ✅ Reliability gap closed: `releaseDiscoverySourceAfterFailedCompletion` now resets cadence fields when the completion transaction fails, ensuring sources exit the `running` state (`server/jobs/discovery/ingest-sources.ts:167-205`, `server/utils/discovery-repository.ts:210-234`).

### Test Review
- ✅ Vitest suite covers string timestamps, worker queues, retry telemetry, thrown adapter errors, and the new completion-failure fallback (`server/jobs/__tests__/discovery-ingest-sources.spec.ts:35-239`).

### Requirements Traceability
- AC1-AC5 satisfied; no remaining gaps identified.

### Risk Summary
- Score: 2 (Low) – Fallback handles transient DB faults; remaining risk limited to upstream instability monitoring.

### NFR / Quality Attribute Observations
- **Reliability:** PASS – sources return to schedulable state even after persistence errors.
- **Maintainability:** PASS – regression guard prevents future omission of the fallback.

### Gate Status
- Gate: PASS (`docs/qa/gates/3.1-ingestion-scheduled-fetch.yml`)

### Recommended Status
- ✓ Ready to Ship – monitor fallback telemetry in production but no further code changes required for this story.
