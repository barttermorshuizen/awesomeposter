# Story 3.1: Scheduled Source Fetching

## Status
Draft

## Story
**As the** discovery ingestion service,
**I want** to fetch client sources on their configured cadence without overlapping runs,
**so that** normalized items stay fresh and reliable.

Refer to `docs/architecture/discovery_agent_backend/jobs-scheduling-and-throughput.md` for ingestion scheduler context and throughput expectations.
Story 3.3 finalizes the retry/backoff policy and is expected to land first; this story assumes its taxonomy and APIs are available.

## Acceptance Criteria
1. Each source stores a fetch interval (default 60 minutes) and scheduler respects it with no overlapping jobs per source.
2. Scheduler selects the appropriate fetch adapter for HTTP pages, RSS feeds, and YouTube channels/playlists based on the source configuration.
3. Concurrent fetches for different sources are allowed but capped to configurable worker pool size.
4. Scheduler records job start/end timestamps and success/failure to telemetry.
5. Failures (network, HTTP status â‰¥400, YouTube API errors) are logged with reason codes and retried according to backoff policy defined in Story 3.3.

## Tasks / Subtasks
- [ ] Implement scheduler using existing job framework (`packages/agents-server` or `server/jobs`) with pluggable adapters per source type (AC 1, AC 2). extend packages/shared/src/discovery/ingestion.ts with new adapter modules under packages/shared/src/discovery/adapters/ and
  register them in the adapter map; no scheduler changes needed beyond wiring new source types into ingest-sources.ts.
- [ ] Persist per-source next-run timestamp in `discovery_sources.next_fetch_at` and update atomically after each job using a single transaction that also records telemetry status (AC 1, AC 4).
- [ ] Add worker pool configuration (env) and guard to prevent overrun (AC 3).
- [ ] Emit telemetry events `ingestion.started` / `ingestion.completed` including source type, source id, duration, run id, and success flag; when failures occur, include `failure_reason` codes aligned with Story 3.3 backoff taxonomy and log structured error records (AC 4, AC 5).
- [ ] Document scheduling behavior in runbook, highlighting RSS and YouTube cadence considerations (AC 1-5).

## Dev Notes
- Follow existing scheduling patterns documented in `docs/architecture/discovery_agent_backend/jobs-scheduling-and-throughput.md` (no new framework required).
- Ensure scheduler is resilient across restarts (persist state in DB, not memory).
- Use the existing `discovery_sources` table for both cadence (`fetch_interval_minutes`) and next run tracking (`next_fetch_at`); initialize missing values when new sources are created.
- On restart, seed the scheduler queue from `next_fetch_at` values to avoid duplicate executions; ensure transactional updates so telemetry + next-run commitment happen together.
- Telemetry contract: emit `ingestion.started` when a job dequeues (`source_id`, `source_type`, `run_id`, `scheduled_at`), and `ingestion.completed` on finish (`duration_ms`, `success`, `failure_reason?`, `retry_in_minutes?`). Log failures with structured context (`error_code`, HTTP status or API error). Align `failure_reason` codes with Story 3.3 backoff taxonomy (`network_error`, `http_4xx`, `http_5xx`, `youtube_quota`, etc.).
- Backoff taxonomy and retry escalation definitions live in `docs/stories/3.3.ingestion-retry-health.md`; reuse its `failure_reason` codes and retry API contracts without deviation.

### Testing
- Integration coverage should include:
  1. Happy-path cadence across multiple sources with staggered `next_fetch_at`, verifying single execution per interval and correct telemetry payloads.
  2. No-overlap guard where a long-running job blocks duplicate scheduling and logs a structured skip message.
  3. Worker-pool limit enforcement (e.g., pool size 2 with three simultaneous sources) showing queued execution order.
  4. Adapter selection validation for HTTP, RSS, and YouTube sources with stubs ensuring correct adapter dispatch.
  5. Failure path logging and telemetry when adapters return network/API errors, including `failure_reason` and `retry_in_minutes` aligned with Story 3.3.
  6. Restart recovery seeding scheduler state from `discovery_sources.next_fetch_at` without duplicates after process restart.
  7. Telemetry contract assertions confirming required fields (`source_id`, `source_type`, `run_id`, `scheduled_at`, `duration_ms`, `success`, optional `failure_reason`/`retry_in_minutes`).
  8. Edge cadence values (minimum/maximum and default 60 min) ensuring intervals persist correctly.
  9. Back-to-back success runs confirming `next_fetch_at` updates and telemetry continuity.
  10. Observability regression ensuring structured logs/metrics still emit as expected.
- Use fake timers and mocked adapters/HTTP responses to simulate long-running schedules and failure conditions.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-03-29 | 0.1 | Draft for Epic: Ingestion & Normalization. | PM |

## Dev Agent Record
_Not yet worked._
