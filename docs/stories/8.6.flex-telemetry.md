# Story 8.6: Flex Telemetry & Logging Parity

## Status
Draft

## Story
**As a** reliability engineer,  
**I want** flex runs to emit structured telemetry and logs aligned with the legacy orchestrator,  
**so that** monitoring dashboards and alerting continue to function when flex flows are enabled.

## Acceptance Criteria
1. Flex server emits `FlexEvent` frames while also logging structured events (start, plan, node, hitl, validation, complete) via Winston with correlation IDs.
2. Metrics counters (HITL requests, validation retries, node duration, completion status) feed existing telemetry pipeline or log-based metrics.
3. Telemetry service exposes in-memory stream or queue for future dashboards, matching naming conventions from legacy server.
4. Documentation lists new event types and log fields so observability teams can update dashboards.

## Tasks / Subtasks
- [ ] Instrument execution engine to log node lifecycle events with correlation/run IDs and capability IDs (AC 1).
- [ ] Emit metrics counters/timers for HITL, retries, and completion outcomes (AC 2).
- [ ] Update SSE payloads to include optional telemetry payloads as needed (without breaking signature) (AC 1, AC 3).
- [ ] Document event/log schema updates in architecture spec and ops runbooks (AC 4).
- [ ] Add tests or snapshots verifying log shape/fields for key events (AC 1).
- [ ] Coordinate with observability team to validate metrics ingestion (AC 2).

## Story Context

**Existing System Integration:**
- Integrates with: Winston logging, current telemetry pipeline, SSE stream consumers.
- Technology: TypeScript, Winston, existing logging middlewares.
- Follows pattern: Legacy agents server log structure and metrics naming.
- Touch points: Execution engine, SSE emission, logging configuration.

## Acceptance Criteria (Detailed)

### Functional Requirements
1. Each `FlexEvent` has a corresponding structured log entry with `eventType`, `runId`, `nodeId`, `capabilityId`, `status`, and `durationMs`.
2. Metrics include counters for `flex.hitl.requested`, `flex.hitl.resolved`, `flex.validation.retry`, `flex.run.complete`, and histograms for node durations.
3. Telemetry service aggregates events for optional streaming to dashboards (placeholder queue/in-memory bus).

### Integration Requirements
4. Logging configuration respects existing env vars (`LOG_LEVEL`, `JSON_LOGS`) and writes to the same sinks.
5. Documentation references new metric names and any required dashboard updates.
6. No regression to legacy logging when flex server is disabled.

### Quality Requirements
7. Snapshot or unit tests ensure log format doesnâ€™t drift (use test logger).
8. SSE frames remain backward compatible (type/id/timestamp/payload).
9. Observability checklist completed with ops confirmation.

## Technical Notes
- **Integration Approach:** Extend existing logging middleware; share telemetry util functions between legacy and flex servers if possible.
- **Existing Pattern Reference:** Legacy orchestrator telemetry code base; replicate naming conventions.
- **Key Constraints:** Avoid double-counting metrics when flex and legacy servers run concurrently; use namespace prefixes (`flex.*`).

## Definition of Done
- [ ] Functional requirements met
- [ ] Integration requirements verified
- [ ] Existing functionality regression tested (legacy logging unaffected)
- [ ] Code follows existing patterns and standards
- [ ] Tests pass (unit/snapshot)
- [ ] Documentation updated (telemetry spec + runbook)

## Risk and Compatibility Check
- **Primary Risk:** Inconsistent log schema confusing dashboards.
- **Mitigation:** Mirror legacy schema, add tests, involve observability team early.
- **Rollback:** Disable flex logging via feature flag or revert instrumentation; legacy monitoring unaffected.

**Compatibility Verification**
- [x] No breaking changes to existing APIs
- [x] Database changes (if any) are additive only
- [x] UI changes follow existing design patterns
- [x] Performance impact is negligible

## Validation Checklist
- [ ] Story can be completed in one development session
- [x] Integration approach is straightforward
- [x] Follows existing patterns exactly
- [x] No design or architecture work required
- [x] Story requirements are unambiguous
- [x] Integration points are clearly specified
- [x] Success criteria are testable
- [x] Rollback approach is simple

## Success Criteria
1. Flex logs/metrics show up in existing dashboards with minimal configuration changes.
2. Operators can trace runs using correlation IDs across logs and SSE streams.
3. Legacy logging behavior remains stable.

## Important Notes
- Coordinate with SRE to schedule dashboard updates once metric names are finalized.
- Consider shipping sample queries (e.g., Loki/Grafana) to expedite observability adoption.
