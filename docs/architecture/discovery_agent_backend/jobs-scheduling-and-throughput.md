# Jobs, Scheduling, and Throughput
- **Triggering**: rely on Nitroâ€™s built-in `crons` configuration (supported in `nitro.config.ts`) to run `discovery-ingest` every 30 minutes per enabled client. For local dev we reuse `npm run dev:api` watchers.
- **Backpressure**: ingestion inspects the volume of unreviewed `scored` items per client (default ceiling 500). When the threshold is exceeded it pauses new fetches, logs a structured warning, and emits a discovery backlog SSE frame so operators can react before reviewers are overwhelmed.
- **Retries**: store fetch errors in `discovery_ingest_runs.metrics_json`. A follow-up job `retry-failed-items.ts` requeues entries flagged as transient failures.
- **Inline scoring**: scoring and dedup run in the same worker pool as normalization. We reuse `withConcurrencyLimit` inside the ingestion job (defaults to 4 parallel item processors) to bound CPU/token usage without a secondary loop.
- **Telemetry**: ingestion metrics track list awareness (`webListApplied`, `listItemCount`) and flow into both `discovery_ingest_runs.metrics_json` and discovery SSE frames so operators can troubleshoot selector efficacy without SQL access.


## Ingestion Pipeline
- **Adapter matrix**: `server/jobs/discovery/ingest-sources.ts` calls adapter helpers defined under `packages/shared/src/discovery/ingestion.ts` and typed by `NormalizedDiscoveryItem` in `packages/shared/src/discovery.ts`.
  - HTTP/JSON sources map to `adapters/http.ts`, which performs `GET` requests with shared headers, throttles by tenant, and validates body schemas before handing normalized payloads into the synchronous processing pipeline.
  - RSS/Atom feeds use `adapters/rss.ts` (driven by `feedparser-promised`) to unwrap entries, canonicalize permalinks, and collapse duplicates by GUID + published timestamp before they hit scoring/dedup.
  - YouTube playlists and channels leverage `adapters/youtube.ts`, preferring the official Data API when credentials exist and falling back to the RSS facade; both normalize into the same shape while capturing `videoId`, `channelId`, and duration metadata for the downstream scoring helpers.
- **Normalization contract**: every adapter returns `{ rawPayload, normalized, sourceMetadata }`; the job records the raw payload in `discovery_items.raw_payload_json`, stores the normalized summary in `normalized_json`, runs scoring + dedup on the normalized payload, and logs adapter metrics/errors into `discovery_ingest_runs.metrics_json` for observability. Rejections bubble an `ingest.error` event before the item is skipped.
- **List extraction mode**: when `webList` is configured the adapter uses the provided container/item selectors, applies configured field mappings with graceful fallbacks to legacy heuristics, and emits one normalized payload per discovered article. The current runtime processes only the first page even if pagination selectors are configured; future work can re-enable multi-page traversal.
- **Worker pool configuration**: ingestion runs per-client batches with `MAX_CONCURRENT_FETCHES` (default 3) enforced inside the job via `p-limit`. Cron cadence (30 minutes) is adjustable per client flag, and the backlog guard now keyes off unreviewed `scored` items instead of a separate queue to keep throughput predictable.
